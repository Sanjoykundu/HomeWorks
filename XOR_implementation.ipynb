import numpy as np
import matplotlib.pyplot as plt
np.random.seed(0)

# dataset for the neural network
gate_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])
expected_output = np.array([[0], [1], [1], [0]])
#print(gate_inputs)
#print(expected_output)
input_neurons, hidden_neurons, output_neurons = 2, 2, 1
epochs = 1000
lr =.05


def sigmoid(x):   #defining sigmoid activation function
    return 1/(1+ np.exp(-x))


def sigmoid_derivative(x):    #defining derivative of sigmoid function for back propagation
    return x*(1-x)


#initializing the weights and biases
input_hidden_weights = np.random.rand(input_neurons,hidden_neurons)
hidden_bias = np.random.rand(1, hidden_neurons)
#hidden_bias = np.array([[1,1]])
hidden_output_weights = np.random.rand(hidden_neurons, output_neurons)
output_bias = np.random.rand(1, output_neurons)
#output_bias = np.array([[1]])

print('Initial input to hidden layer weights:')
print(input_hidden_weights)
print('Initial hidden layer bias:')
print(hidden_bias)
print('Initial hidden to poutput layer weights:')
print(hidden_output_weights)
print('Initial output layer bias:' )
print(output_bias)


for i in range (epochs):
    #feed forward calculation
    hidden_layer_input = np.matmul(gate_inputs, input_hidden_weights) + hidden_bias
    hidden_layer_output = sigmoid(hidden_layer_input)
    output_layer_input = np.matmul(hidden_layer_output, hidden_output_weights) + output_bias
    output_layer_output = sigmoid(output_layer_input)
    #print(hidden_layer_output.shape)

    
    #back Propagation
    error = 0.5 * (expected_output - output_layer_output) **2
    temp =-(expected_output - output_layer_output)
    output_layer_derivative = temp * sigmoid_derivative(output_layer_output)
    error_hidden_layer = np.matmul(output_layer_derivative, hidden_output_weights.T)
    hidden_layer_derivative = error_hidden_layer * sigmoid_derivative(hidden_layer_output)
                
    hidden_output_weights -= np.matmul(hidden_layer_output.T, output_layer_derivative) * lr
    output_bias -= np.sum(output_layer_derivative, axis=0, keepdims= True) * lr
    input_hidden_weights -= np.matmul(gate_inputs.T,hidden_layer_derivative) * lr
    hidden_bias -= np.sum(hidden_layer_derivative, axis=0, keepdims= True) * lr
    #print("this is iteration: ", i)
    #print("error value", error)
    #print("Input to Hidden layer weights : ", input_hidden_weights)
    #print("Hidden layer to Output layer Weights: ", hidden_output_weights)
    #print("Hidden layer bias: ", hidden_bias)
    #print("Output layer bias: ", output_bias)
    #print("Final output: ", output_layer_output)
 
print("error value",error)
print("Input to Hidden layer weights : ",input_hidden_weights)
print("Hidden layer to Output layer Weights: ",hidden_output_weights)
print("Hidden layer bias: ",hidden_bias)
print("Output layer bias: ",output_bias)
print("Final output: ",output_layer_output)
